import asyncio
import logging
import os
import secrets
import subprocess
import tempfile
from pathlib import Path

from openai import AsyncOpenAI
from openai import OpenAIError
from aiogram import Bot, Dispatcher, F
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart
from aiogram.types import (
    CallbackQuery,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    Message,
)
from dotenv import load_dotenv
import imageio_ffmpeg


load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
FFMPEG_BIN = os.getenv("FFMPEG_BIN", "ffmpeg")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_TRANSCRIBE_MODEL = os.getenv("OPENAI_TRANSCRIBE_MODEL", "gpt-4o-transcribe")
OPENAI_TRANSCRIBE_LANGUAGE = os.getenv("OPENAI_TRANSCRIBE_LANGUAGE", "ru").strip()
OPENAI_ORG = os.getenv("OPENAI_ORG")
openai_client: AsyncOpenAI | None = None
SUMMARY_AI_PROMPT = (
    "ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ ÑÑƒÑ‚ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹. "
    "Ð£Ð±ÐµÑ€Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ñ‹, ÑÐ¼Ð¾Ñ†Ð¸Ð¸ Ð¸ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð½ÑƒÑŽ Ð²Ð¾Ð´Ñƒ. ÐŸÐ¸ÑˆÐ¸ ÑÑÐ½Ð¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ."
)


logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

if not BOT_TOKEN:
    raise RuntimeError("BOT_TOKEN is missing. Set it in the environment or .env file.")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY is missing. Set it in the environment or .env file.")


bot = Bot(BOT_TOKEN, parse_mode=ParseMode.MARKDOWN)
dp = Dispatcher()
bot_username_cache: str | None = None
pending_requests: dict[str, dict[str, int | str]] = {}


async def get_bot_username() -> str:
    global bot_username_cache
    if bot_username_cache:
        return bot_username_cache
    me = await bot.get_me()
    bot_username_cache = me.username or "voice_to_text_bot"
    return bot_username_cache


async def download_voice_file(file_id: str, dest: Path) -> None:
    file = await bot.get_file(file_id)
    await bot.download_file(file.file_path, destination=dest)


def resolve_ffmpeg_bin() -> str:
    if FFMPEG_BIN and FFMPEG_BIN != "auto":
        return FFMPEG_BIN
    return imageio_ffmpeg.get_ffmpeg_exe()


def convert_to_wav(src: Path, dest: Path) -> None:
    ffmpeg_bin = resolve_ffmpeg_bin()
    cmd = [
        ffmpeg_bin,
        "-y",
        "-i",
        str(src),
        "-ar",
        "16000",
        "-ac",
        "1",
        str(dest),
    ]
    logger.debug("Running ffmpeg: %s", " ".join(cmd))
    completed = subprocess.run(cmd, capture_output=True)
    if completed.returncode != 0:
        raise RuntimeError(
            f"ffmpeg failed ({completed.returncode}): {completed.stderr.decode(errors='ignore')}"
        )


def get_openai_client() -> AsyncOpenAI:
    global openai_client
    if openai_client is None:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY, organization=OPENAI_ORG)
    return openai_client


async def transcribe_audio(wav_path: Path, prompt: str | None = None) -> str:
    request: dict[str, str] = {
        "model": OPENAI_TRANSCRIBE_MODEL,
    }
    if OPENAI_TRANSCRIBE_LANGUAGE:
        request["language"] = OPENAI_TRANSCRIBE_LANGUAGE
    if prompt:
        request["prompt"] = prompt

    with wav_path.open("rb") as audio_file:
        transcription = await get_openai_client().audio.transcriptions.create(
            file=audio_file,
            **request,
        )
    text = getattr(transcription, "text", None)
    if isinstance(text, str):
        return text.strip()
    if isinstance(transcription, str):
        return transcription.strip()
    raise RuntimeError("Unexpected transcription response format from OpenAI API")


def build_start_keyboard(bot_username: str) -> InlineKeyboardMarkup:
    invite_link = f"https://t.me/{bot_username}?startgroup=true"
    return InlineKeyboardMarkup(
        inline_keyboard=[
            [InlineKeyboardButton(text="Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð±Ð¾Ñ‚Ð° Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ", url=invite_link)],
        ]
    )


@dp.message(CommandStart())
async def handle_start(message: Message) -> None:
    bot_username = await get_bot_username()
    keyboard = build_start_keyboard(bot_username)
    await message.answer(
        "ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÑÑ‚Ð¾ Ð±Ð¾Ñ‚ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ Ð² Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚.\n"
        "ÐŸÑ€Ð¾ÑÑ‚Ð¾ Ð¿ÐµÑ€ÐµÑˆÐ»Ð¸ Ð»ÑŽÐ±Ð¾Ðµ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¾Ñ‚ Ð´Ñ€ÑƒÐ³Ð°, Ð¸ Ñ ÐµÐ³Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€ÑƒÑŽ!\n"
        "ÐœÐµÐ½Ñ Ñ‚Ð°ÐºÐ¶Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹!",
        reply_markup=keyboard,
    )


@dp.message(F.voice)
async def handle_voice(message: Message) -> None:
    if message.chat.type in {"group", "supergroup"}:
        status_message = await message.reply("Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽ Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²ÐºÑƒ...")
        await transcribe_and_send(
            file_id=message.voice.file_id,
            status_message=status_message,
            mode="full",
        )
        return

    request_id = secrets.token_hex(6)
    pending_requests[request_id] = {"file_id": message.voice.file_id, "user_id": message.from_user.id}
    keyboard = InlineKeyboardMarkup(
        inline_keyboard=[
            [InlineKeyboardButton(text="ÐŸÐ¾Ð»Ð½Ð°Ñ Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²ÐºÐ°", callback_data=f"tr:full:{request_id}")],
            [InlineKeyboardButton(text="Summary AI", callback_data=f"tr:summary:{request_id}")],
        ]
    )
    await message.reply("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð²Ð¸Ð´:", reply_markup=keyboard)


async def summarize_text(text: str) -> tuple[str, bool]:

    cleaned = (text or "").strip()
    if not cleaned:
        return "Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½.", False

    try:
        user_content = f"{SUMMARY_AI_PROMPT}\n\nÐ Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²ÐºÐ°:\n{cleaned}"

        completion = await get_openai_client().chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "Ð¢Ñ‹ â€” Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð´Ð»Ñ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ ÑÑƒÑ‚Ð¸."},
                {"role": "user", "content": user_content},
            ],
            temperature=0.2,
            max_completion_tokens=200,
        )
        choice = (completion.choices[0].message.content or "").strip()
        return (choice or "Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½."), True
    except OpenAIError as exc:  # pragma: no cover - network dependent
        logger.warning("OpenAI summary failed, fallback to local: %s", exc)
    except Exception as exc:  # pragma: no cover - network dependent
        logger.warning("Unexpected OpenAI error, fallback to local: %s", exc)

    sentences: list[str] = []
    current = []
    for ch in cleaned:
        current.append(ch)
        if ch in {".", "!", "?"}:
            sentences.append("".join(current).strip())
            current = []
        if len(sentences) >= 2:
            break
    if not sentences and current:
        sentences.append("".join(current).strip())
    summary = " ".join(sentences) if sentences else cleaned
    return (summary[:800] or "Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½."), False


async def transcribe_and_send(file_id: str, status_message: Message, mode: str) -> None:
    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_dir_path = Path(tmp_dir)
            ogg_path = tmp_dir_path / "voice.ogg"
            wav_path = tmp_dir_path / "voice.wav"

            await download_voice_file(file_id, ogg_path)
            convert_to_wav(ogg_path, wav_path)
            transcribe_prompt = SUMMARY_AI_PROMPT if mode == "summary" else None
            transcript = await transcribe_audio(wav_path, prompt=transcribe_prompt)

        transcript = transcript or "Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½."
        if mode == "summary":
            summary, used_openai = await summarize_text(transcript)
            safe = (
                summary.replace("`", "\\`")
                .replace("*", "\\*")
                .replace("_", "\\_")
            )
            suffix = "" if used_openai else "\n_(Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð·, OpenAI Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)_"
            await status_message.edit_text(f"ðŸ¤–Summary AI:\n**{safe}**{suffix}")
        else:
            safe_text = transcript.replace("`", "\\`")
            await status_message.edit_text(f"```\n{safe_text}\n```")
    except Exception as exc:
        logger.exception("Failed to transcribe voice: %s", exc)
        await status_message.edit_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ðµ Ñ€Ð°Ð· Ð¿Ð¾Ð·Ð¶Ðµ.")


@dp.callback_query(F.data.startswith("tr:"))
async def handle_choice(callback: CallbackQuery) -> None:
    await callback.answer()

    try:
        _, mode, request_id = callback.data.split(":", 2)
    except ValueError:
        await callback.message.edit_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð²Ñ‹Ð±Ð¾Ñ€. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð·.")
        return

    payload = pending_requests.pop(request_id, None)
    if not payload:
        await callback.message.edit_text("Ð—Ð°Ð¿Ñ€Ð¾Ñ ÑƒÑÑ‚Ð°Ñ€ÐµÐ». ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð·.")
        return

    expected_user = payload.get("user_id")
    if expected_user and callback.from_user and callback.from_user.id != expected_user:
        await callback.message.answer("Ð­Ñ‚Ð¾ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ðµ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚ Ð´Ñ€ÑƒÐ³Ð¾Ð¼Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŽ. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒ ÑÐ²Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ.")
        return

    status_message = await callback.message.edit_text("Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽ Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²ÐºÑƒ...")
    await transcribe_and_send(
        file_id=str(payload["file_id"]),
        status_message=status_message,
        mode=mode,
    )


async def main() -> None:
    await dp.start_polling(bot, allowed_updates=dp.resolve_used_update_types())


if __name__ == "__main__":
    asyncio.run(main())
